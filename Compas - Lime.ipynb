{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d87665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from lime import lime_tabular\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import math\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e76af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'score_factor ~ Q(\"priors_count\") + Q(\"two_year_recid\") + Q(\"crime_factor\") + Q(\"age_cat_25 - 45\") + Q(\"age_cat_Greater than 45\") + Q(\"age_cat_Less than 25\") + Q(\"race_African-American\") + Q(\"race_Asian\") + Q(\"race_Caucasian\") + Q(\"race_Hispanic\") + Q(\"race_Native American\") + Q(\"race_Other\") + Q(\"sex_Female\") + Q(\"sex_Male\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d165461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = sm.GLM.from_formula(formula, family=sm.families.Binomial(), data=df)\n",
    "resultado = modelo.fit()\n",
    "resultado.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32daca4e",
   "metadata": {},
   "source": [
    "# Interpretabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96230fba",
   "metadata": {},
   "source": [
    "La interpretabilidad es el grado en que un ser humano puede comprender la causa de una decisión. También se puede definir como el grado en que un ser humano puede predecir consistentemente el resultado del modelo. [Interpretability](https://christophm.github.io/interpretable-ml-book/interpretability.html)\n",
    "\n",
    "En [Explainable AI: An illuminator in the field of black-box machine learning](https://towardsdatascience.com/explainable-ai-an-illuminator-in-the-field-of-black-box-machine-learning-62d805d54a7a) se explican brevemente algunos métodos. Para mayor detalle, [Molnar](https://christophm.github.io/interpretable-ml-book/index.html) describen más métodos de interpretación de modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f47313d",
   "metadata": {},
   "source": [
    "## Lime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c8fcfb",
   "metadata": {},
   "source": [
    "'LIME' significa `Local Interpretable Model agnostic Explanations` que toma cualquier modelo de aprendizaje automático como entrada y genera explicaciones sobre las contribuciones de las características al hacer una predicción. Asume que es un modelo de caja negra, lo que significa que no conoce el funcionamiento interno de los modelos y genera una explicación basada en esta suposición. [How to Use LIME to Understand sklearn Models Predictions?](https://coderzcolumn.com/tutorials/machine-learning/how-to-use-lime-to-understand-sklearn-models-predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b9d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valores_salida = ['Low', 'HM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetasX = ['priors_count', 'two_year_recid', 'crime_factor', 'age_cat_25 - 45',\n",
    "       'age_cat_Greater than 45', 'age_cat_Less than 25',\n",
    "       'race_African-American', 'race_Asian', 'race_Caucasian',\n",
    "       'race_Hispanic', 'race_Native American', 'race_Other', 'sex_Female',\n",
    "       'sex_Male']\n",
    "etiquetaY = ['score_factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70cd37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[etiquetasX].to_numpy()\n",
    "Y = df[etiquetaY].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e2fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.90, test_size=0.1, stratify=Y, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4363f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test  Accuracy : %.2f\"%lr.score(X_test, Y_test))\n",
    "print(\"Train Accuracy : %.2f\"%lr.score(X_train, Y_train))\n",
    "print()\n",
    "print(\"Confusion Matrix : \")\n",
    "print(confusion_matrix(Y_test, lr.predict(X_test)))\n",
    "print()\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(Y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfe881",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_tabular.LimeTabularExplainer(X_train, mode=\"classification\",\n",
    "                                              class_names=valores_salida,\n",
    "                                              feature_names=df.columns.to_list(),\n",
    "                                             )\n",
    "\n",
    "explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24dd8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(1, len(X_test))\n",
    "\n",
    "print(\"Prediction : \", valores_salida[lr.predict(X_test[idx].reshape(1,-1))[0]])\n",
    "print(\"Actual :     \", valores_salida[Y_test[idx][0]])\n",
    "\n",
    "explanation = explainer.explain_instance(X_test[idx], lr.predict_proba,\n",
    "                                         num_features=len(df.columns.to_list()))\n",
    "\n",
    "explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38072956",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "Molnar, C. (2022). Interpretable Machine Learning. Retrieved 18 February 2022, from https://christophm.github.io/interpretable-ml-book/index.html\n",
    "\n",
    "How to Use LIME to Understand sklearn Models Predictions [Python]? by Sunny Solanki. (2022). Retrieved 18 February 2022, from https://coderzcolumn.com/tutorials/machine-learning/how-to-use-lime-to-understand-sklearn-models-predictions\n",
    "\n",
    "Explainable AI: An illuminator in the field of black-box machine learning. (2021). Retrieved 18 February 2022, from https://towardsdatascience.com/explainable-ai-an-illuminator-in-the-field-of-black-box-machine-learning-62d805d54a7a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
